"""
Data Collector for Trading System
==================================
æ”¯æŒå¤šå€‹è³‡æ–™æºï¼š
- Binance APIï¼ˆåŠ å¯†è²¨å¹£ï¼Œç„¡éœ€ API Keyï¼‰
- yfinanceï¼ˆç¾è‚¡ï¼‰
"""

import os
import sys
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List
import time

import pandas as pd
import yfinance as yf
import requests
from dotenv import load_dotenv

# åŠ è¼‰ç’°å¢ƒè®Šæ•¸
load_dotenv('file.env')

# é…ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class BinanceDataCollector:
    """
    å¾å¹£å®‰å…¬é–‹ API æ”¶é›†åŠ å¯†è²¨å¹£è³‡æ–™
    ä¸éœ€è¦ API Keyï¼Œå®Œå…¨å…è²»
    """

    def __init__(self):
        """åˆå§‹åŒ–å¹£å®‰å…¬é–‹ API"""
        self.base_url = 'https://api.binance.com/api/v3/klines'
        self.data_dir = Path('data/crypto')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Crypto data directory: {self.data_dir.absolute()}")

        # æ™‚é–“æ¡†æ¶è½‰æ›ç‚ºå¹£å®‰æ ¼å¼
        self.timeframe_map = {
            '15m': '15m',
            '1h': '1h',
            '4h': '4h',
            '1d': '1d',
        }

    def fetch_klines(
        self,
        symbol: str,
        interval: str,
        days: int = 90
    ) -> pd.DataFrame:
        """
        å¾å¹£å®‰å…¬é–‹ API ç²å– K ç·šè³‡æ–™
        """
        try:
            logger.info(f"  â†“ Fetching {symbol} {interval}...")

            # è¨ˆç®—é–‹å§‹æ™‚é–“
            start_time = int((datetime.now() - timedelta(days=days)).timestamp() * 1000)

            # å¹£å®‰å…¬é–‹ API åƒæ•¸
            params = {
                'symbol': symbol,
                'interval': self.timeframe_map[interval],
                'startTime': start_time,
                'limit': 1000
            }

            all_klines = []

            # åˆ†æ‰¹ç²å–
            while True:
                response = requests.get(self.base_url, params=params, timeout=10)
                response.raise_for_status()

                klines = response.json()

                if not klines:
                    break

                all_klines.extend(klines)

                # æ›´æ–°é–‹å§‹æ™‚é–“
                params['startTime'] = klines[-1][0] + 1

                # é¿å…è¢«é™æµ
                time.sleep(0.05)

                if len(klines) < 1000:
                    break

            if not all_klines:
                logger.warning(f"    âœ— No data for {symbol}")
                return pd.DataFrame()

            # è½‰æ›ç‚º DataFrame
            df = pd.DataFrame(all_klines, columns=[
                'open_time', 'open', 'high', 'low', 'close', 'volume',
                'close_time', 'quote_asset_volume', 'number_of_trades',
                'taker_buy_base_asset_volume', 'taker_buy_quote_asset_volume', 'ignore'
            ])

            # æ¸…ç†è³‡æ–™
            df['timestamp'] = pd.to_datetime(df['open_time'], unit='ms')
            df = df[['timestamp', 'open', 'high', 'low', 'close', 'volume']]

            # è½‰æ›ç‚ºæµ®é»æ•¸
            for col in ['open', 'high', 'low', 'close', 'volume']:
                df[col] = pd.to_numeric(df[col], errors='coerce')

            df = df.dropna()

            logger.info(f"    âœ“ Got {len(df)} candles")
            return df

        except Exception as e:
            logger.error(f"    âœ— Error: {e}")
            return pd.DataFrame()

    def save_csv(self, df: pd.DataFrame, symbol: str, interval: str) -> bool:
        """ä¿å­˜ç‚º CSV"""
        try:
            filepath = self.data_dir / f"{symbol}_{interval}.csv"
            df.to_csv(filepath, index=False)
            logger.info(f"    ğŸ’¾ Saved to {filepath.name}")
            return True
        except Exception as e:
            logger.error(f"    âœ— Save error: {e}")
            return False

    def collect_all(self, pairs: List[str], intervals: List[str]) -> int:
        """æ”¶é›†æ‰€æœ‰äº¤æ˜“å°å’Œæ™‚é–“æ¡†æ¶çš„è³‡æ–™ï¼Œè¿”å›æˆåŠŸæ•¸"""
        logger.info(f"\nğŸ“Š Binance Crypto Data Collection")
        logger.info(f"  Pairs: {', '.join(pairs)}")
        logger.info(f"  Intervals: {', '.join(intervals)}")
        logger.info("-" * 60)

        success_count = 0

        for symbol in pairs:
            logger.info(f"\n  {symbol}:")
            for interval in intervals:
                df = self.fetch_klines(symbol, interval)

                if not df.empty and self.save_csv(df, symbol, interval):
                    success_count += 1

        return success_count


class YFinanceDataCollector:
    """å¾ yfinance æ”¶é›†ç¾è‚¡è³‡æ–™ - ç°¡åŒ–ç‰ˆæœ¬"""

    def __init__(self):
        """åˆå§‹åŒ–"""
        self.data_dir = Path('data/stock')
        self.data_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"Stock data directory: {self.data_dir.absolute()}")

    def fetch_stock(
            self,
            symbol: str,
            interval: str,
            days: int = 90
    ) -> pd.DataFrame:
        """
        å¾ yfinance ç²å–è‚¡ç¥¨è³‡æ–™
        """
        try:
            logger.info(f"  â†“ Fetching {symbol} for {days} days with {interval} interval...")

            df = yf.download(
                symbol,
                period=f"{days}d",
                interval=interval,
                auto_adjust=True
            )

            if df is None or df.empty:
                logger.warning(f"    âœ— No data for {symbol}")
                return pd.DataFrame()

            logger.info(f"    âœ“ Downloaded {len(df)} rows, processing...")

            # é‡ç½®ç´¢å¼•ï¼ˆé€™æœƒæŠŠ Date/Datetime è®Šæˆä¸€åˆ—ï¼‰
            df = df.reset_index()

            # ç”¨ä½ç½®ç´¢å¼•è€Œä¸æ˜¯åˆ—åä¾†è¨ªå•ï¼ˆå®Œå…¨é¿å…åˆ—åå•é¡Œï¼‰
            try:
                # é€šå¸¸çµæ§‹æ˜¯ï¼šDate/Datetime, Open, High, Low, Close, Volume, ...
                # æˆ‘å€‘ç›´æ¥ç”¨ç¬¬ 0-5 åˆ—
                result_df = pd.DataFrame()

                # ç¬¬ 0 åˆ—ï¼šæ™‚é–“æˆ³
                result_df['timestamp'] = df.iloc[:, 0]

                # ç¬¬ 1-5 åˆ—ï¼šOHLCV
                result_df['open'] = df.iloc[:, 1]
                result_df['high'] = df.iloc[:, 2]
                result_df['low'] = df.iloc[:, 3]
                result_df['close'] = df.iloc[:, 4]
                result_df['volume'] = df.iloc[:, 5]

                # è½‰æ›æ™‚é–“æˆ³
                result_df['timestamp'] = pd.to_datetime(result_df['timestamp'])

                # è½‰æ›æ•¸å€¼
                for col in ['open', 'high', 'low', 'close', 'volume']:
                    result_df[col] = pd.to_numeric(result_df[col], errors='coerce')

                # ç§»é™¤ NaN
                result_df = result_df.dropna()

                if result_df.empty:
                    logger.warning(f"    âœ— No valid data for {symbol}")
                    return pd.DataFrame()

                logger.info(f"    âœ“ Got {len(result_df)} valid candles")
                return result_df

            except Exception as e:
                logger.error(f"    âœ— Error processing columns: {e}")
                logger.error(f"    DataFrame shape: {df.shape}, columns: {list(df.columns)}")
                return pd.DataFrame()

        except Exception as e:
            logger.error(f"    âœ— Error fetching {symbol}: {e}")
            return pd.DataFrame()

    def save_csv(self, df: pd.DataFrame, symbol: str, interval: str) -> bool:
        """ä¿å­˜ç‚º CSV"""
        try:
            filepath = self.data_dir / f"{symbol}_{interval}.csv"
            df.to_csv(filepath, index=False)
            logger.info(f"    ğŸ’¾ Saved to {filepath.name}")
            return True
        except Exception as e:
            logger.error(f"    âœ— Save error: {e}")
            return False

    def collect_all(self, symbols: List[str], intervals: List[str]) -> int:
        """æ”¶é›†æ‰€æœ‰è‚¡ç¥¨å’Œæ™‚é–“æ¡†æ¶çš„è³‡æ–™"""
        logger.info(f"\nğŸ“ˆ yfinance Stock Data Collection")
        logger.info(f"  Symbols: {', '.join(symbols)}")
        logger.info(f"  Intervals: {', '.join(intervals)}")
        logger.info("-" * 60)

        success_count = 0

        for symbol in symbols:
            logger.info(f"\n  Processing {symbol}:")
            for interval in intervals:
                df = self.fetch_stock(symbol, interval)

                if not df.empty and self.save_csv(df, symbol, interval):
                    success_count += 1

        return success_count


def main():
    """ä¸»ç¨‹å¼"""
    try:
        logger.info("=" * 70)
        logger.info("ğŸš€ Trading Data Collection System")
        logger.info("=" * 70)

        # ç¡¬ç·¨ç¢¼é…ç½®
        crypto_pairs = ['BTCUSDT', 'ETHUSDT', 'SOLUSDT', 'XRPUSDT', 'ADAUSDT',
                        'DOGEUSDT', 'AVAXUSDT', 'LINKUSDT', 'DOTUSDT', 'LTCUSDT']
        crypto_timeframes = ['15m', '1h', '4h']

        stock_pairs = ['SBUX', 'KO', 'AMZN', 'AAPL', 'TSLA',
                       'NVDA', 'MSFT', 'GOOGL', 'META', 'JPM']
        stock_timeframes = ['1h', '1d']  # æ”¹æˆåªæŠ“ 1h å’Œ 1dï¼ˆ15m æ²’æœ‰æ­·å²è³‡æ–™ï¼‰

        # æ”¶é›†åŠ å¯†è²¨å¹£è³‡æ–™
        crypto_collector = BinanceDataCollector()
        crypto_success = crypto_collector.collect_all(crypto_pairs, crypto_timeframes)

        # æ”¶é›†ç¾è‚¡è³‡æ–™
        stock_collector = YFinanceDataCollector()
        stock_success = stock_collector.collect_all(stock_pairs, stock_timeframes)

        # ç¸½çµ
        total = len(crypto_pairs) * len(crypto_timeframes) + len(stock_pairs) * len(stock_timeframes)
        success = crypto_success + stock_success

        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… Data Collection Complete!")
        logger.info(f"   Success: {success}/{total}")
        logger.info(f"   Crypto: {crypto_success} files")
        logger.info(f"   Stock:  {stock_success} files")
        logger.info("=" * 70)

    except Exception as e:
        logger.error(f"âŒ Fatal error: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == '__main__':
    main()
